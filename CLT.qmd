---
title: "CLT"
author: "Visruth Srimath Kandali"
format: html
embed-resources: true
# engine: julia
---

## Creating the Data

```{julia}
#| output: false
using Distributions, Random, DataFrames, CSV

function t_statistic(data, μ, σ)
    n = length(data)
    (mean(data) - μ) / (σ / sqrt(n))
end
```

Herein we create a simple function to calculate the t-value of some data given μ, σ. We will use this in our analysis later on to see if, analogous to the classically defined Central Limit Theorem, the sampling distribution of t-values can be well approximated by a t-distribution.

```{julia}
#| output: false
function analysis(statistic, distro, n, r, params...)
    Random.seed!(0)

    d = distro(params...)
    μ = mean(d)
    σ = std(d)

    sample_statistics = zeros(r)
    Threads.@threads for i in 1:r
        sample_statistics[i] = try
            statistic(rand(d, n), μ, σ)
        catch
            statistic(rand(d, n))
        end
    end

    m = mean(sample_statistics)
    s = std(sample_statistics)

    critical = statistic == mean ? quantile(Normal(), 0.975) : quantile(TDist(n - 1), 0.975)

    upper = sum(sample_statistics .>= m + critical * s) / r
    lower = sum(sample_statistics .<= m - critical * s) / r

    (upper, lower, upper + lower, upper - lower)
end
```

This function creates a sampling distribution, using the given distribution and its parameters. It does this `r` times to mitigate any concerns of natural sampling variability. The function then finds the upper and lower tails of this sampling distribution under the assumption that it is normally distributed. The function then returns the tail weights along with their sum and difference. If this sampling distribution was approximately Gaussian, we would always expect tail weights of roughly 0.025, a sum of 0.05, and a difference of 0.

```{julia}
#| output: false
function analyze_distributions(statistic, r)
    println("Analyzing distributions with $(r) repetitions")

    sample_sizes = [5, 10, 20, 30, 40, 100, 200, 300, 400, 1000, 2000, 3000, 4000, 5000, 10000]
    distributions = [
        (LogNormal, [0, 1.4865], 31.65266),
        (Poisson, [0.001], 31.6228),
        (Gamma, [0.02], 20),
        (LogNormal, [0, 1], 6.1849),
        (Exponential, [], 2),
        (LogNormal, [0, 0.5], 1.7502),
        (LogNormal, [0, 0.25], 0.7883),
        (Beta, [0.3, 0.2], -0.4),
        (Normal, [], 0)
    ]
    results = DataFrame(Distribution=String[], Params=Vector[], Skewness=Float64[], Sample_Size=Int[], Upper_Tail=Float64[], Lower_Tail=Float64[], Tail_Sum=Float64[], Tail_Difference=Float64[])

    for (distro, params, skewness) in distributions
        println("$(distro) with parameters $(params)")
        for n in sample_sizes
            upper, lower, tail_sum, tail_diff = analysis(statistic, distro, n, r, params...)
            push!(results, (string(distro), params, skewness, n, upper, lower, tail_sum, tail_diff))
        end
    end

    results
end

```

We chose to look at a variety of distributions with varying levels of skewness, which can be seen in `distributions`, and across various samples sizes as shown in `sample_sizes`.

```{julia}
#| output: false
analyze_distributions(mean, 1) # compile

results = analyze_distributions(mean, 100000)
CSV.write("CLT_means.csv", results)

results = analyze_distributions(t_statistic, 100000)
CSV.write("CLT_t.csv", results)
```

Here we use 100,000 repetitions to create our sampling distributions of means and of t-statistics.

## Graphing Results

```{r setup}
library(tidyverse)
df_means <- read.csv("CLT_means.csv")
df_t <- read.csv("CLT_t_sigma.csv")
```

```{r}
df_means_clean <- df_means |>
  mutate(
    Combined = paste(Distribution, Params, Skewness, sep = " "),
    Combined = fct(Combined)
  ) |>
  group_by(Combined)

for (combined in unique(df_means_clean$Combined)) {
  df_means_clean |>
    filter(Combined == combined) |>
    ggplot(aes(x = Sample_Size, y = Tail_Sum)) +
    geom_line() +
    geom_hline(yintercept = 0.05, linetype = "dashed") +
    labs(title = paste0("Central Limit Theorem:", combined, sep = " "), x = "Sample Size", y = "Tail Sum") +
    ylim(0.03, 0.053)

  ggsave(paste0("Plots/CLT_", combined, ".png"), width = 6, height = 4)
}
```

```{r}
for (combined in unique(df_means_clean$Combined)) {
  df_means_clean |>
    filter(Combined == combined) |>
    ggplot(aes(x = Sample_Size, y = Tail_Sum)) +
    geom_line() +
    geom_hline(yintercept = 0.05, linetype = "dashed") +
    labs(title = paste0("Tail Sum as Sample Size Increases", combined, sep = " "), x = "Sample Size", y = "Tail Sum") +
    ylim(0.042, 0.053)

  ggsave(path = "Plots/Total", filename = paste0(combined, ".png"), width = 6, height = 4)
}

df_means_clean |>
  filter(Distribution == "Poisson") |>
  ggplot(aes(x = Sample_Size, y = Tail_Sum)) +
  geom_line() +
  geom_hline(yintercept = 0.05, linetype = "dashed") +
  labs(title = paste0("Tail Sum as Sample Size Increases", combined, sep = " "), x = "Sample Size", y = "Tail Sum")

ggsave(path = "Plots/Total", filename = "Poisson [0.001] 31.6228.png", width = 6, height = 4)

# ggplot(df_means_clean, aes(x = Sample_Size, y = Tail_Sum)) +
#   geom_line() +
#   geom_hline(yintercept = 0.05, linetype = "dashed") +
#   labs(title = "Central Limit Theorem: Means", x = "Sample Size", y = "Tail Sum")
```

```{r}
for (combined in unique(df_means_clean$Combined)) {
  df_means_clean |>
    filter(Combined == combined) |>
    ggplot(aes(x = Sample_Size)) +
    geom_line(aes(y = Lower_Tail)) +
    geom_line(aes(y = Upper_Tail)) +
    geom_hline(yintercept = 0.025, linetype = "dashed") +
    labs(title = paste0("Tails as Sample Size Increases", combined, sep = " "), x = "Sample Size", y = "Tails")

  ggsave(path = "Plots/Tails", filename = paste0(combined, ".png"), width = 6, height = 4)
}
```

```{r}
for (combined in unique(df_means_clean$Combined)) {
  df_means_clean |>
    filter(Combined == combined) |>
    mutate(Tail_Difference = abs(Tail_Difference)) |>
    ggplot(aes(x = Sample_Size, y = Tail_Difference)) +
    geom_line() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = paste0("Tail Difference as Sample Size Increases", combined, sep = " "), x = "Sample Size", y = "Tail Sum")

  ggsave(path = "Plots/Diff", filename = paste0(combined, ".png"), width = 6, height = 4)
}
```

TODO: With Quarto 1.5 use native Julia engine

```{r}
df_means |>
  mutate(test = (1 + Skewness) / sqrt(Sample_Size), .after = Tail_Difference) |>
  filter(Tail_Sum >= 0.048, Tail_Difference <= 0.015) |>
  # get the maximum test value for each distribution
  group_by(Distribution) |>
  summarize(Max_Test = max(test)) |>
  arrange(desc(Max_Test)) |>
  View()
```

```{r}
# df_means |>
#   filter(str_detect(Distribution, "LogNormal")) |>
#   ggplot(aes(x = Sample_Size, y = Tail_Sum)) +
#   geom_line() +
#   geom_hline(yintercept = 0.05, linetype = "dashed") +
#   labs(title = "Central Limit Theorem: LogNormal", x = "Sample Size", y = "Tail Sum") +
#   ylim(0.03, 0.053) +
#   facet_wrap(~Skewness)

df_means |>
  filter(str_detect(Distribution, "Normal"), Skewness < 10) |>
  ggplot(aes(x = Sample_Size, y = Tail_Sum)) +
  geom_line(aes(y = Lower_Tail, color = "red")) +
  geom_line(aes(y = Upper_Tail, color = "blue")) +
  geom_hline(yintercept = 0.025, linetype = "dashed") +
  labs(title = "Central Limit Theorem: Normal", x = "Sample Size", y = "Tail Sum") +
  facet_wrap(~Skewness)
```

```{r}
read.csv("test.csv") |>
  mutate(test = (1 + Skewness) / sqrt(Sample_Size), .after = Tail_Difference) |>
  arrange(Distribution, Sample_Size) |>
  filter(str_detect(Distribution, "Normal"), Skewness < 10) |>
  View()
  # ggplot(aes(x = Sample_Size, y = Tail_Sum)) +
  # ylim(0, 0.1) +
  # geom_line() +
  # # geom_line(aes(y = Tail_Sum, color = "red")) +
  # geom_hline(yintercept = 0.05, linetype = "dashed") +
  # labs(title = "Central Limit Theorem: Normal", x = "Sample Size", y = "Tail Sum") |>
  # facet_wrap(~Skewness)
```
