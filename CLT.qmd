---
title: "Effects of Skewness on Sample Size in CLT"
author: "Visruth Srimath Kandali, Beth Chance, California Polytechnic San Luis Obispo Department of Statistics"
format: html
embed-resources: true
# engine: julia
---

## Creating the Data

```{julia sampling distribution}
#| output: false
using Distributions, Random, DataFrames, CSV, StatsBase

function standardize(x̄, μ, σ, n::Int64)::Float64
    (x̄ - μ) / (σ / sqrt(n))
end

function sampling_distribution(statistic::Function, d::Distribution, n::Int, r::Int)::Vector{Float64}
    Random.seed!(0)
    sample_statistics = zeros(r)
    sample = zeros(n)

    # Sampling r times and calculating the statistic
    @inbounds for i in 1:r
        rand!(d, sample)
        sample_statistics[i] = statistic(sample)
    end

    sample_statistics
end
```

This function creates a sampling distribution, using the given distribution and its parameters. It takes samples of size `n` and calculates each sample's `statistic`. It does this \`r\`\` times to create a sampling distribution and mitigate any concerns of natural sampling variability. It returns this sampling distribution for further analysis

```{julia Single distribution analysis}
#| output: false
function analysis(statistic, d::Distribution, n::Int, r::Int, μ::Real, σ::Real, sample_statistics=sampling_distribution(statistic, d, n, r))::Tuple{Float64,Float64,Float64}
    sample_statistics = sampling_distribution(statistic, d, n, r)

    skewness = StatsBase.skewness(sample_statistics)

    z_scores = standardize.(sample_statistics, μ, σ, n)
    upper = sum(z_scores .>= 1.96) / r
    lower = sum(z_scores .<= -1.96) / r

    (upper, lower, skewness)
end
```

This function then finds the upper and lower tails of this sampling distribution under the assumption that it is normally distributed by looking at z-scores more extreme than $\pm$ 1.96. The function then returns the tail weights along with the skewness of the sampling distribution.

```{julia Analyzing multiple distributions}
#| output: false
function analyze_distributions(statistic, r::Int64, sample_sizes::Vector{Int64}, distributions)::DataFrame
    println("Analyzing distributions with $(r) repetitions")
    # Some setup
    results = DataFrame(
        "Distribution" => String[],
        "Skewness" => Float64[],
        "Sample Size" => Int64[],
        "Upper Tail" => Float64[],
        "Lower Tail" => Float64[],
        "Sampling Skewness" => Float64[],
        "Population Mean" => Float64[],
        "Population SD" => Float64[]
    )
    # Analyzing each distribution
    @inbounds for d::Distribution in distributions
        # # Getting population parameters
        μ = mean(d)
        σ = std(d)
        skewness = StatsBase.skewness(d)

        # # Analyzing each sample size
        u = Threads.SpinLock() # lock to avoid data races
        @inbounds Threads.@threads for n in sample_sizes
            upper, lower, sample_skew = analysis(statistic, d, n, r, μ, σ)
            Threads.lock(u) do
                push!(results, (string(d), skewness, n, upper, lower, sample_skew, μ, σ))
            end
        end
    end

    results
end
```

We chose to look at a variety of distributions with varying levels of skewness across a range of sample sizes. We selected these parameters to showcase the effects of skewness across differing distributions as well as the effects of skewness within the same distribution, and of course across sample size.

```{julia Generate data}
#| output: false
function main()
    sample_sizes = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300, 400, 500]
    distributions = [
        Gamma(16),
        LogNormal(0, 0.25),
        Gamma(4),
        Gamma(2),
        LogNormal(0, 0.5),
        Gamma(1),
        Exponential(),
        Gamma(0.64),
        LogNormal(0, 0.75)
    ]
    # Compile
    analyze_distributions(mean, 1, sample_sizes, distributions)

    # ~10 minutes on a i7-12700h (10,000,000 repetitions used in the report)
    # results::DataFrame = analyze_distributions(mean, 10_000_000, sample_sizes, distributions)

    # ~1 minute on a i7-12700h
    results::DataFrame = analyze_distributions(mean, 1_000_000, sample_sizes, distributions)

    CSV.write("means.csv", results)
    nothing
end

main()
```

Here we use r = 1,000,000 repetitions to create our sampling distributions of means. Keep in mind that, while Julia is fast, this is still a non-trivial task and will take some time. It is highly recommended to run the code in a Julia REPL instead of this notebook.

## Analyzing the Data

```{r setup}
#| message: false
library(tidyverse)
library(flextable)

df <- read_csv("means.csv") |>
  group_by(Distribution) |>
  mutate(Distribution = str_replace(Distribution, "\\{.*\\}", " ")) |>
  filter(
    `Lower Tail` >= 0.02 & `Lower Tail` <= 0.03,
    `Upper Tail` >= 0.02 & `Upper Tail` <= 0.03
  ) |>
  filter(`Sample Size` == min(`Sample Size`)) |> # get the smallest sample size for each distribution
  select(Distribution, Skewness, `Sampling Skewness`, `Sample Size`) |>
  arrange(Skewness, Distribution)

ft <- flextable(df) |>
  colformat_double(j = c("Skewness", "Sampling Skewness"), digits = 3) |>
  font(fontname = "Trebuchet MS") |>
  autofit()

save_as_image(ft, path = "Poster//table.png", res = 2000)
ft
```

```{r Fitting the model}
model <- lm(Skewness ~ sqrt(`Sample Size`), df)
df |>
  ggplot(aes(x = sqrt(`Sample Size`), y = Skewness, label = Distribution)) +
  geom_abline(slope = coef(model)[2], intercept = coef(model)[1], color = "blue", linetype = "dashed", linewidth = 1.5) +
  geom_line(linewidth = 1.5) +
  geom_point(size = 3) +
  ggrepel::geom_label_repel(seed = 0, nudge_x = 1, size = 5) +
  labs(
    title = "Linear Relationship between Skewness and Empirical Minimum Square Root Sample Size",
    x = "Square Root of Sample Size",
    y = "Skewness"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(size = 23),
    axis.title = element_text(size = 17),
    axis.text = element_text(size = 12)
  )

ggsave("Poster//Skew_Sample_Size.png", width = 15, height = 8.5, dpi = 1000)

summary(model)
```

## Generating Some Graphs

```{julia Generating Exponential data for graphing}
#| output: false
d = Exponential()
μ = mean(d)
σ = std(d)
n = 30

exponential30 = sampling_distribution(mean, d, n, 10_000_000)
exponential30std = standardize.(exponential30, μ, σ, n)

n = 150
exponential150 = sampling_distribution(mean, d, n, 10_000_000)
exponential150std = standardize.(exponential150, μ, σ, n)

graphing = DataFrame(
    "Exponential" => exponential30,
    "Exponential Z-Scores" => exponential30std,
    "Exponential 150" => exponential150,
    "Exponential 150 Z-Scores" => exponential150std
)

# Warning: this produces a large file (~700MB) which is why you will not find graphing.csv in the repository
CSV.write("graphing.csv", graphing)
```

```{r Graphing sampling distributions}
#| message: false
graphing <- read_csv("graphing.csv") |>
  select("Exponential Z-Scores", "Exponential 150 Z-Scores")

# Create exponential population plot
x <- seq(0, 5, length.out = 100)
y <- dexp(x, rate = 1)
sub <- ggplot() +
  geom_line(aes(x, y)) +
  labs(
    title = "Exponential Population with Rate = 1"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(size = 12),
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank()
  )

for (col_name in colnames(graphing)) {
  # Determine sample size
  sample_size <- ifelse(grepl("150", col_name), "n = 150", "n = 30")

  # Calculate upper and lower bounds
  upper <- sum(graphing[[col_name]] >= 1.96) / 1000000
  lower <- sum(graphing[[col_name]] <= -1.96) / 1000000

  # Main plot
  main <- ggplot(graphing, aes(x = .data[[col_name]])) +
    geom_histogram(bins = 100, aes(y = after_stat(density))) +
    stat_function(
      fun = dnorm,
      args = list(mean = mean(graphing[[col_name]]), sd = sd(graphing[[col_name]])),
      color = "blue",
      linewidth = 1.25
    ) +
    geom_vline(xintercept = 1.96, linetype = "dashed", color = "blue") +
    geom_vline(xintercept = -1.96, linetype = "dashed", color = "blue") +
    labs(
      title = paste("Sampling Distribution of Standardized Means for", sample_size),
      x = "Z Score",
      y = "Frequency"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(size = 20),
      axis.title = element_text(size = 17),
      axis.text = element_text(size = 12)
    )

  # Combine plots
  combined <- main + patchwork::inset_element(sub, 0.6, 0.6, 1, 1)

  # Save the plot
  ggsave(paste0("Poster//", col_name, ".png"), plot = combined, width = 12, height = 6.75, dpi = 1000)
}
```

```{r Graphing tails}
read_csv("means.csv") |>
  group_by(Distribution) |>
  mutate(Distribution = paste(str_replace(Distribution, "\\{.*\\}", " "), round(Skewness, 2))) |>
  ggplot(aes(x = `Sample Size`, color = Distribution)) +
  geom_rect(
    aes(xmin = 0, xmax = Inf, ymin = 0.02, ymax = 0.03),
    fill = "grey",
    linewidth = 0
  ) +
  geom_hline(yintercept = 0.025, linetype = "dashed", linewidth = 1) +
  geom_line(aes(y = `Upper Tail`), linewidth = 1) +
  geom_line(aes(y = `Lower Tail`), linewidth = 1) +
  geom_vline(xintercept = 30, linetype = "dashed", linewidth = 0.75) +
  annotate("text", x = 40, y = 0.04, label = "n = 30", hjust = 0, size = 5, color = "black") +
  labs(
    title = "Upper and Lower Tail Weights of Sampling Distributions",
    x = "Sample Size",
    y = "Tail Weight"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(size = 23),
    axis.title = element_text(size = 17),
    axis.text = element_text(size = 12)
  )

ggsave("Poster//Tail_Weights.png", width = 12, height = 6.75, dpi = 1000)
```