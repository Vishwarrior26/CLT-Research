---
title: "Effects of Skewness on Sample Size in CLT"
author: "Visruth Srimath Kandali. Beth Chance, California Polytechnic Department of Statistics"
format: 
    html:
        df-print: kable
embed-resources: true
# engine: julia
---

## Creating the Data

```{julia Single distribution analysis}
#| output: false
using Distributions, Random, DataFrames, CSV, StatsBase

function analysis(statistic::Function, d::Distribution, n::Int64, r::Int64, μ::Number)
    Random.seed!(0)
    sample_statistics = zeros(r)
    sample = zeros(n)

    @inbounds for i in 1:r
        rand!(d, sample)
        if statistic == mean
            sample_statistics[i] = statistic(sample)
        elseif statistic == t_statistic
            sample_statistics[i] = statistic(sample, μ)
        end
    end

    skewness = StatsBase.skewness(sample_statistics)

    μ::Float64 = mean(d)
    σ::Float64 = std(d)

    z_scores = (sample_statistics .- μ) ./ (σ / sqrt(n))
    upper = sum(z_scores .>= 1.96) / r
    lower = sum(z_scores .<= -1.96) / r

    (upper, lower, skewness)
end
```

This function creates a sampling distribution, using the given distribution and its parameters. It does this `r` times to mitigate any concerns of natural sampling variability. The function then finds the upper and lower tails of this sampling distribution under the assumption that it is normally distributed by looking at z-scores more extreme than $\pm$ 1.96. The function then returns the tail weights along with the skewness of the sampling distribution.

```{julia Perform analysis on distributions}
#| output: false
function analyze_distributions(statistic::Function, r::Number)::DataFrame
    println("Analyzing distributions with $(r) repetitions")

    sample_sizes = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300, 400, 500]
    distributions = [
        Gamma(16),
        LogNormal(0, 0.25),
        Gamma(4),
        Gamma(2),
        LogNormal(0, 0.5),
        Gamma(1),
        Exponential(),
        LogNormal(0, 0.75)
    ]

    results = DataFrame(
        Distribution=String[],
        Skewness=Float64[],
        Sample_Size=Int[],
        Upper_Tail=Float64[],
        Lower_Tail=Float64[],
        Sampling_Skewness=Float64[],
        Population_Mean=Float64[],
        Population_SD=Float64[]
    )

    @inbounds for d::Distribution in distributions
        println("$(string(d))")

        μ::Float64 = mean(d)
        σ::Float64 = std(d)
        skewness::Float64 = StatsBase.skewness(d)

        u = Threads.SpinLock()
        @inbounds Threads.@threads for n in sample_sizes
            upper, lower, sample_skew = analysis(statistic, d, n, r, μ)
            Threads.lock(u) do
                push!(results, (string(d), skewness, n, upper, lower, sample_skew, μ, σ))
            end
        end

    end

    sort!(results, [:Distribution, :Sample_Size])
end
```

We chose to look at a variety of distributions with varying levels of skewness, which can be seen in `distributions`, and across various samples sizes as shown in `sample_sizes`. We selected these distributions to showcase the effects of skewness across differing distributions as well as the effects of skewness within the same distribution.

```{julia Generate data}
#| output: false
# compiling the function
analyze_distributions(mean, 1)

# results = analyze_distributions(mean, 1_000_000)
results = analyze_distributions(mean, 1000)
CSV.write("means.csv", results)
```

Here we use r = 1,000,000 repetitions to create our sampling distributions of means. Keep in mind that, while Julia is fast, this is still a non-trivial task and will take some time. It is highly recommended to run the code in a Julia REPL instead of this notebook.

## Analyzing the Data

```{r setup}
#| message: false
library(tidyverse)
library(ggrepel)
library(flextable)

df <- read_csv("means.csv") |>
  group_by(Distribution) |>
  mutate(Distribution = str_replace(Distribution, "\\{.*\\}", " ")) |>
  filter(
    `Lower Tail` >= 0.02 & `Lower Tail` <= 0.03,
    `Upper Tail` >= 0.02 & `Upper Tail` <= 0.03
  ) |>
  filter(`Sample Size` == min(`Sample Size`)) |> # get the smallest sample size for each distribution
  select(Distribution, Skewness, `Sampling Skewness`, `Sample Size`) |>
  arrange(Skewness, Distribution)

save_as_image(flextable(df), "table.png")

```

```{r Fitting the model}
model <- lm(Skewness ~ sqrt(`Sample Size`), df)
df |>
  ggplot(aes(x = sqrt(`Sample Size`), y = Skewness, label = Distribution)) +
  geom_abline(slope = coef(model)[2], intercept = coef(model)[1], color = "blue", linetype = "dashed") +
  geom_line() +
  geom_point() +
  geom_label_repel() +
  labs(
    title = "Linear Relationship between Skewness and Square Root Sample Size",
    x = "Square Root of Sample Size",
    y = "Skewness"
  )  +
  theme_bw()

summary(model)
```

```{r graphing}
set.seed(0)

# Generate random values from a normal distribution
data <- rexp(1000)

df <- data.frame(value = data)

# Create a histogram using ggplot2
ggplot(data = df, aes(x = value)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black", aes(y = after_stat(density))) +
  labs(
    title = "Histogram of Random Values from Exponential Distribution",
    x = "Values",
    y = "Density"
  )
```
