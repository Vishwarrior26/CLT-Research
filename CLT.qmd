---
10---
title: "CLT"
author: "Visruth Srimath Kandali"
format: html
embed-resources: true
# engine: julia
---

## Creating the Data

```{julia}
#| output: false
using Distributions, Random, DataFrames, CSV

function analysis(statistic, distro, n, r, params...)
    Random.seed!(0)

    d = distro(params...)
    μ = mean(d)
    σ = std(d)

    sample_statistics = zeros(r)
    Threads.@threads for i in 1:r
        sample_statistics[i] = try
            statistic(rand(d, n), μ, σ)
        catch
            statistic(rand(d, n))
        end
    end

    m = mean(sample_statistics)
    s = std(sample_statistics)

    critical = statistic == mean ? quantile(Normal(), 0.975) : quantile(TDist(n - 1), 0.975)

    upper = sum(sample_statistics .>= m + critical * s) / r
    lower = sum(sample_statistics .<= m - critical * s) / r

    (upper, lower, upper + lower, upper - lower)
end
```

This function creates a sampling distribution, using the given distribution and its parameters. It does this `r` times to mitigate any concerns of natural sampling variability. The function then finds the upper and lower tails of this sampling distribution under the assumption that it is normally distributed. The function then returns the tail weights along with their sum and difference. If this sampling distribution was approximately Gaussian, we would always expect tail weights of roughly 0.025, a sum of 0.05, and a difference of 0.

```{julia}
#| output: false
function analyze_distributions(statistic, r)
    println("Analyzing distributions with $(r) repetitions")

    sample_sizes = [5, 10, 20, 30, 40, 100, 200, 300, 400, 1000, 2000, 3000, 4000, 5000, 10000]
    distributions = [
        (LogNormal, [0, 1.4865], 31.65266),
        (Poisson, [0.001], 31.6228),
        (Gamma, [0.02], 20),
        (LogNormal, [0, 1], 6.1849),
        (Exponential, [], 2),
        (LogNormal, [0, 0.5], 1.7502),
        (LogNormal, [0, 0.25], 0.7883),
        (Beta, [0.3, 0.2], -0.4),
        (Normal, [], 0)
    ]
    results = DataFrame(Distribution=String[], Params=Vector[], Skewness=Float64[], Sample_Size=Int[], Upper_Tail=Float64[], Lower_Tail=Float64[], Tail_Sum=Float64[], Tail_Difference=Float64[])

    for (distro, params, skewness) in distributions
        println("$(distro) with parameters $(params)")
        for n in sample_sizes
            upper, lower, tail_sum, tail_diff = analysis(statistic, distro, n, r, params...)
            push!(results, (string(distro), params, skewness, n, upper, lower, tail_sum, tail_diff))
        end
    end

    results
end

```

We chose to look at a variety of distributions with varying levels of skewness, which can be seen in `distributions`, and across various samples sizes as shown in `sample_sizes`.

```{julia}
#| output: false
# compiling the function
analyze_distributions(mean, 1)

results = analyze_distributions(mean, 1000000)
CSV.write("means.csv", results)
```

Here we use 1,000,000 repetitions to create our sampling distributions of means.

## Graphing Results

```{r setup}
library(tidyverse)
df_means <- read.csv("means.csv")
```

```{r}
df <- read.csv("means.csv") |>
  group_by(Distribution) |>
  filter(
    Lower_Tail >= 0.02 & Lower_Tail <= 0.03,
    Upper_Tail >= 0.02 & Upper_Tail <= 0.03
  ) |> 
  filter(Sample_Size == min(Sample_Size)) |> # get the smallest sample size for each distribution
  select(Distribution, Skewness, Sampling_Skewness, Sample_Size) |>
  arrange(Skewness, Distribution)

model <- lm(Skewness ~ sqrt(Sample_Size), df)

df |>
  ggplot(aes(x = sqrt(Sample_Size), y = Skewness)) +
  # geom_abline(slope = sqrt(exp(1))/10, intercept = coef(model)[1], color = "red", linetype = "dashed") +
  geom_abline(slope = coef(model)[2], intercept = coef(model)[1], color = "blue", linetype = "dashed") +
  geom_line() +
  labs(title = "Linear Relationship between Skewness and Square Root Sample Size", 
       x = "Square Root of Sample Size", 
       y = "Skewness")

print(df) # pander?
print(model)
```
